{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature filtering based on Mutual Information for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# depending on the OS the path to the data file is different\n",
    "if os.name == 'nt':\n",
    "    data_nutr = pd.read_excel(r'..\\data\\nutrient-file-release2-jan22.xlsx', sheet_name='All solids & liquids per 100g')\n",
    "elif os.name == 'posix':\n",
    "    data_nutr = pd.read_excel(r'../data/nutrient-file-release2-jan22.xlsx', sheet_name='All solids & liquids per 100g')\n",
    "\n",
    "# dataframe with only data columns\n",
    "\n",
    "\n",
    "# print first 5 results.\n",
    "data_nutr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is largely continuous, we need to turn features into discrete ones.\n",
    "\n",
    "## Variable discretisation\n",
    "\n",
    "There are different methods to discretise continuous variables:\n",
    "\n",
    "- **Equal-width binning**: divides the scope of possible values into N bins of the same width.\n",
    "- **Equal-frequency binning**: divides the scope of possible values into N bins, each of them containing approximately the same number of samples.\n",
    "- **Domain knowledge binning**: divides the scope of possible values into bins according to the domain knowledge.\n",
    "<!-- There are other methods too listed below.\n",
    "\n",
    "- ChiMerge: merges the bins using the Chi2 test to evaluate the statistical dependence of the classes and the feature.\n",
    "- Entropy-based binning: merges the bins using the entropy of the classes and the feature.\n",
    "- K-means binning: merges the bins using the K-means algorithm.\n",
    "- Gaussian mixture binning: merges the bins using a Gaussian Mixture Model.\n",
    "- Quantile binning: merges the bins so that each bin contains the same number of samples.\n",
    "- Uniform binning: merges the bins so that each bin contains the same width.\n",
    "- Recursive partitioning: merges the bins using a decision tree.\n",
    "- Discretisation using decision trees: merges the bins using a decision tree.\n",
    "- Discretisation using clustering: merges the bins using a clustering algorithm.\n",
    "- Discretisation using support vector machines: merges the bins using a support vector machine.\n",
    "- Discretisation using linear models: merges the bins using a linear model.\n",
    "- Discretisation using nearest neighbours: merges the bins using a nearest neighbours algorithm.\n",
    "- Discretisation using kernel density estimation: merges the bins using a kernel density estimation.\n",
    "- Discretisation using fuzzy logic: merges the bins using a fuzzy logic algorithm.\n",
    "- Discretisation using genetic algorithms: merges the bins using a genetic algorithm.\n",
    "- Discretisation using simulated annealing: merges the bins using a simulated annealing algorithm.\n",
    "- Discretisation using a neural network: merges the bins using a neural network.\n",
    "- Discretisation using a random forest: merges the bins using a random forest.\n",
    "- Discretisation using a linear discriminant analysis: merges the bins using a linear discriminant analysis.\n",
    "- Discretisation using a quadratic discriminant analysis: merges the bins using a quadratic discriminant analysis.\n",
    "- Discretisation using a principal component analysis: merges the bins using a principal component analysis.\n",
    "- Discretisation using a factor analysis: merges the bins using a factor analysis.\n",
    "- Discretisation using a canonical correlation analysis: merges the bins using a canonical correlation analysis.\n",
    "- Discretisation using a partial least squares regression: merges the bins using a partial least squares regression.\n",
    "- Discretisation using a ridge regression: merges the bins using a ridge regression. -->\n",
    "\n",
    "WARNING: The choice of bins will influence the results of the mutual information filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable discretisation using pandas.qcut\n",
    "\n",
    "# add new column with discretised values\n",
    "data_nutr['Discretised Energy with dietary fibre, equated \\n(kJ)'] = pd.cut(data_nutr['Energy with dietary fibre, equated \\n(kJ)'], 20, labels=False)\n",
    "\n",
    "# print the first few rows of the data for the two columns\n",
    "data_nutr[['Energy with dietary fibre, equated \\n(kJ)', 'Discretised Energy with dietary fibre, equated \\n(kJ)']].head(10)\n",
    "\n",
    "data_nutr.head()\n",
    "# print the first few rows of the sorted data for the two columns\n",
    "# data[['Energy with dietary fibre, equated \\n(kJ)', 'Discretised Energy with dietary fibre, equated \\n(kJ)']].sort_values(by='Energy with dietary fibre, equated \\n(kJ)', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretise all columns of data\n",
    "\n",
    "\n",
    "\n",
    "ignored_columns = ['Public Food Key', 'Classification', 'Food Name']\n",
    "label = 'Classification' # label to test\n",
    "test_col = [] # names of columns\n",
    "data_nutr = data_nutr.fillna(0) # having values of NaN prevents calculation of MI scores.\n",
    "\n",
    "for nutrient in data_nutr.columns:\n",
    "    if nutrient in ignored_columns: \n",
    "        continue # disregard first 3 columns ['Public Food Key', 'Classification', 'Food Name']\n",
    "    else:\n",
    "        test_col.append(nutrient) # for features below\n",
    "        data_nutr[nutrient] = pd.cut(data_nutr[nutrient], 20, labels=False) # issues with pd.qcut relating to size of bins, proceeded with pd.cut\n",
    "        # discretise each column so data is discrete and not continuous\n",
    "\n",
    "# as follows in Week 9 Workshop - Feature filtering based on Mutual Information for classification\n",
    "features = data_nutr[test_col]\n",
    "features = features.fillna(0)\n",
    "class_label = data_nutr[label]\n",
    "\n",
    "data_nutr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Following in Week 9 Workshop - Feature filtering based on Mutual Information for classification\n",
    "filtered_features = []\n",
    "THRESHOLD = 0.2 # threshold value not fixed\n",
    "\n",
    "mi_arr = mutual_info_classif(X=features, y=class_label, discrete_features=True)\n",
    "\n",
    "\n",
    "for feature, mi in zip(features.columns, mi_arr):\n",
    "    print(f'MI value for feature \"{feature}\": {mi:.4f}')\n",
    "\n",
    "    if (mi >= THRESHOLD):\n",
    "        filtered_features.append(feature)\n",
    "\n",
    "print('\\nFeature set after filtering with MI:', filtered_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
