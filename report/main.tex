\documentclass[11pt]{article}

\usepackage{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[colorlinks=true, linkcolor=black, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{minted}
\usepackage{subfig}
\setcitestyle{aysep={,}}



\title{Predicting Food Groups \\ \vspace{0.5em} \large from nutritional information}

\author{
    David Sha\\
\AND
    Victoria Lyngaae\\
\AND
    Mason Sebek\\
\AND
    William Spongberg\\
\AND
\AND
	COMP20008: Elements of Data Processing\\
\AND
	The University of Melbourne\\
}

\date{May 2023}

\renewcommand{\headeright}{Predicting Food Groups from Nutritional Information}
\renewcommand{\undertitle}{Report}
\renewcommand{\shorttitle}{}

\begin{document}
\maketitle

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\section{Introduction}

In this report, we will be analysing a food dataset from \cite{FoodStandardsAustraliaNewZealand}. Using the $k$-nearest neighbours algorithm, we build a model that predicts the food groups of a particular food item based on its nutritional information and explore how useful this model is to the food industry.

% The objective of this project is to wrangle and analyse data contained within the \cite{FoodStandardsAustraliaNewZealand} dataset, for the purpose of answering an explicit and defined research question. We propose the inquiry: Can we predict the food group of a food item, based solely on its nutritional information? We implemented the supervised learning model of $k$-nearest neighbours to predict and explore the relationship between nutritional information and food group classification.

\subsection{Research Question}
% 1. What is the research question?
To what extent can we predict the food group of a food item based solely on its nutritional information?

\subsection{Target Audience}
% 2. Who are the target audience.
The outcome of this report may be interesting to major supermarket chains such as Coles or Woolworths. If the model is sufficiently accurate, it could be useful in detecting food items that may or may not be labelled correctly or are missing labels in a particular batch of food items sent to a store. Better classification of food items would improve a supermarket's inventory management as well as its product placement, which will ultimately improve the shopping experience of customers.

% The findings of this exploration may be of value to a number of audiences, such as nutritionists, health researchers, consumers, educational institutions and food retailers.  An effective food group classification model has many practical applications such as:
% \begin{itemize}
%     \item Detection of food items that are labelled incorrectly, either by accident or with the intent to mislead consumers
%     \item Improving a retailer's inventory management and product placement, and ultimately a customer's shopping experience
%     \item Effective assessment of an individual's nutritional intake and dietary pattern
%     \item Educating and enhancing public understanding of food groups, and general nutritional literacy
% \end{itemize}



\subsection{Dataset}
% 3. The dataset you have chosen.
We make use of the dataset from \cite{FoodStandardsAustraliaNewZealand} containing 1616 food items and their nutritional information. This dataset contains all the information we need to answer our research question as each food item has a \verb|Classification| (food group) and respective nutritional information (such as energy, protein, etc.). We also use a complementary dataset from \cite{FoodClassification} which maps numerical classifications to their respective food groups which becomes very useful when providing some domain context during preprocessing and analysis.

\section{Methodology}

\emph{What wrangling and analysis methods (including at least one supervised learning method) have you applied? Why have you chosen these methods over other alternatives? How do you perform the experiment?}

% TODO: rewrite this paragraph
During the initial stages of our investigation, we debated over linear regression on a continuous variable such as \verb|Energy with dietary fibre, equated (kJ)| or $k$-nearest neighbours on a categorical variable such as \verb|Classification|. We explored both options and found it more interesting and more valuable to tackle the latter. All data wrangling techniques are implemented in \verb|preprocessing.ipynb| and all supervised learning and analysis techniques are implemented in \verb|knn.ipynb|.
% During the first stages of our investigation, we explored different target variables given any subset of the nutritional information. We initially considered predicting the \verb|Classification| of a food item but found that the number of unique classifications was too large to be useful. Then we pivoted for some time toward predicting the \verb|Energy|

\subsection{Data Wrangling}

\subsubsection{Preprocessing}

Prior to any use of machine learning, we evaluated  the quality of the data and found that it was not clean. Thus, we implemented various preprocessing techniques to ensure that the data was suitable to be included in the data pipeline. 


A large number of food items were wholly missing particular nutrition values. Understanding that this is likely an indication that a food has either an absence of the nutrient (eg, 0g) or that the food has not been checked for the nutrient's presence, we chose to set all NaN values to 0. Next, given that Food Standards Australia New Zealand (2013) indicates that the first two digits of a food's \verb|Classification| denote its major food group, each item's classification was simplified to these two characters. Doing so reduced the number of unique classifications drastically (from 290 to 23), which made our $k$-nn supervised learning model more effective later on. Given that there are now 23 unique food groups, as seen in Figure \ref{fig:food-group-distribution}, we also chose to simplify and compress less populated food groups, i.e., those containing very few food items, into a broader group, named \verb|Miscellaneous|.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{report/figs/number-of-rows-per-food-group.png}
    \caption{The distribution of food items in twenty-three unique food groups not including unclassified items in the \cite{FoodStandardsAustraliaNewZealand} dataset.}
    \label{fig:food-group-distribution}
\end{figure}

However, rather than arbitrarily choosing a number of food groups to be condensed into \verb|Miscellaneous| (group 31), we opted to explore 3 different versions of the miscellaneous grouping; with 23, 14, and 6 groups (including the miscellaneous group) respectively condensed into \verb|Miscellaneous|. This can also be understood as providing the machine learning model with 6, 14 and 23 total labels, respectively for each variation.

\subsubsection{Feature Selection}

We also utilised \verb|mutual_info_classif| from \verb|sklearn|'s feature selection Python module to evaluate which nutritional features contained the most useful information for classification. With each food item having up to 290 different nutritional measures, the data is highly dimensional, and thus, through feature selection, we can improve the effectiveness of the $k$-nn model. With the mutual information calculated, by choosing an appropriate threshold (\verb|THRESHOLD=0.2| in \verb|preprocessing.ipynb|), we could remove some features that were not particularly insightful. This was done for each set of food groups, to ensure that they could be compared fairly.

We had initially considered using linear regression, but opted to continue our investigation using $k$-nn, as our research question required that our target variable be categorical/discrete, rather than continuous.  This is why, in \verb|preprocessing.ipynb|, we show working for both \verb|./archive/linear-regression.ipynb| and \verb|./src/knn.ipynb|, but only include the latter in the \verb|src| directory.

\subsection{Supervised Learning}

\subsubsection{Training and Test Split}

With our data preprocessed and cleaned, we decided to use an 85\% to 15\% train/test split (\verb|TEST_SPLIT=0.15| in \verb|knn.py|). The model was created using only the \verb|train| data, to ensure that the model could be fairly tested and evaluated based on its performance on unseen data, thus avoiding overfitting and ensuring generalisation.

% TODO: where to know the percentages?
We also employed a 10-fold cross validation approach to optimise the value of $k$ for each set of food groups. Calculating the mean accuracy of the cross validated $k$-nn model for a wide range of $k$ values (Figure \ref{fig:knn-cross-validation}) revealed that a $k$ of 1 performed best for each variation. Given that a $k$ value of 1 may potentially cause the model to be overfitted, as stated from \cite{StackExchangePost}, it was decided to err on the side of caution and opt for each variation's next best $k$, especially given this is not wholly detrimental to the model's accuracy. This procedure can be seen in detail in \verb|knn.py| under the $k$-nn heading in the first function \verb|cross_validate()|.

\begin{figure}[htbp]
    \centering
    \subfloat[\centering 6 food groups]{{\includegraphics[height=5cm]{report/figs/knn-cross-validation-first.png} }}
    \qquad
    \subfloat[\centering 14 food groups]{{\includegraphics[height=5cm]{report/figs/knn-cross-validation-second.png} }}
    \qquad
    \subfloat[\centering 23 food groups]{{\includegraphics[height=5cm]{report/figs/knn-cross-validation-third.png} }}
    \caption{The $k$-nn cross validation mean accuracy for the 6, 14 and 23 food group classification models.}
    \label{fig:knn-cross-validation}
\end{figure}

Continuing with each variation's optimal, we also evaluated the final $k$-nn models using cross validation. By splitting the test data into 7 folds, (to somewhat emulate the 85\% to 15\% training and test split), we were able to evaluate whether our model could consistently and accurately predict a food item's food group, even when trained and tested on different sections of the preprocessed data; i.e., verify that the initial success of the model was not a fluke.

The data was also evaluated using bootstrap validation, where the $k$-nn model's performance was measured against multiple random subsets of the original data, called bootstrap samples. Bootstrap samples were created using randomly selected data points and the $k$-nn model trained upon them 1000 times, the high number of samples allowing for more accurate performance metrics. This validation reduces bias introduced by $k$-nn from the single training-test split and enhances the model's accuracy assessment by ensuring a more robust and unique evaluation every time it is run.

\section{Results}
\emph{What are the key results your research has obtained?}

\SaveVerb{topx}|TOP_X|
\begin{figure}[htbp]
    \centering
    \subfloat[\centering 6 food groups]{{\includegraphics[height=6cm]{report/figs/knn-confusion-matrix-first.png} }}
    \qquad
    \subfloat[\centering 14 food groups]{{\includegraphics[height=6cm]{report/figs/knn-confusion-matrix-second.png} }}
    \qquad
    \subfloat[\centering 23 food groups]{{\includegraphics[height=6cm]{report/figs/knn-confusion-matrix-third.png} }}
    \caption{The confusion matrices for the \protect\UseVerb{topx} food group models.}
    \label{fig:confusion-matrices}
\end{figure}

% OUTDATED
In our analysis, we managed to achieve an accuracy score of 90.12\% (maybe not accurate?). Quite a high accuracy score suggests that our model can very accurately predict the classification of a food given its nutritional information. It is critical to note that during our evaluation process using 10-fold cross-validation, we witnessed a steep drop in our accuracy metrics, seeing accuracy range from 21.67\% to 65.94\%.
\section{Discussion}
\emph{Why are your results significant and valuable?}

Given our results have shown that our machine learning model has achieved an accuracy score of 90.12\%, it suggests that our model can very accurately predict the classification of a good given its nutritional information.(Add bit discussing benefits this can imply for use, specifically regarding our research question).

Interpretation of the results (What are the key results your research has obtained? Why are your results significant and valuable?)
Limitations and future improvements

DISCUSSION POINTS (not exhaustive):
3 variations of the span of miscellaneous group - elaborate on why we chose to do this (because it explores how preprocessing decisions affects the model etc, and because it shows the relationship between nutritional values and food groups and because some groups have few food items, elaborate more than this

Discuss what is shown in the graphs, compare the results of the three variations

If bootstrap accuracies is still much higher than cross validation and initial accuracy values, speculate why this has happened, research and refer to chatgpt screenshot on discord to help explain this
Refer back to what people understand to be a food group, what the outcome of this investigation tells us at the end of the day, elaborate on the significance and justification of the results

Interpret the results: WHAT DO THEY TELL US

Important - discuss thoroughly the limitations of the model, REFER TO RUBRIC FOR POINTS TO COVER


%response
In this study, one part of the pre-processing caught our eye: the number of food groups to be ingested by the miscellaneous group. Three different variations of the data were created, focusing on the top 6, 13 and 23 largest food groups [can correct me here if I’m describing them incorrectly]. As the number of food groups increased, the accuracy of predicting the specific food group for each item decreased. This indicates that the model performs better in categorising foods into larger, more general food groups, but struggles with smaller, more specialised groups. This brings about a limitation of the model, and into question its accuracy. This analysis highlights how post-processing techniques can significantly influence the data and the resulting predictions, demonstrating the importance of careful consideration and understanding of the potential effects of post-processing methods.

The study found that the predictive model is highly accurate in determining food groups based on a food’s nutritional values, giving an 80\%-90\% range across the three different methods of accuracy verification. In the bootstrap validation, the high precision of 75\%-88\% ensures reliable positive predictions while the high recall of 70\%-90\% guarantees accurate identification of data. The high F1 score indicates that the model is successful in achieving both high precision and high recall, making it a reliable tool predicting food groups based on nutritional information.
[Further discussion of the cross-fold validation results here, of which I am unsure how to describe]

However, these high accuracy rates still allow margins for error, which could be problematic considering the vast number of food items found in e.g. supermarkets. If there was an error of roughly 15\% allowed, as our data suggests, it would still require a great deal of manual data checking which would be time-consuming and challenging due to the large volume of items. It brings into question whether this model is worth the potential time and money saved, when there is simply going to have to be time and money spent regardless just to check for errors.

We also came across the potential issue of overfitting in the bootstrap validation technique. It shows that the bootstrap accuracy might be higher than the cross-fold accuracy, likely due to the training data being overused in the resampling process, which could have led to overfitting. However, the high cross-fold validation accuracy has been used to mitigate this concern, providing confidence that the dataset can be effectively utilised for training the model.
[Is the cross-fold validation trustworthy enough to be used in place of the bootstrap validation here?]




\section{Conclusion}
\emph{What are the limitations of your results and how can the project be improved for future?}

Throughout this investigation, we gained valuable insights into the data processing pipeline. The processes and strategies we utilised in preprocessing and analysis of the data elucidate the relationship between nutritional values and food groupings. With the ultimate goal of exploring and answering the question of ‘to what extent can we predict the food group of a food item based solely on its nutritional information?’,  this research endeavour has demonstrated this is entirely possible to achieve. It has also shown that the effectiveness of this enterprise is dependent on not only the initial data available, but also highly contingent on early-stages data cleaning and preparation. The validation and evaluation we conducted on our knn models supportively indicate that this is generally a successful way of predicting food groups, despite the aforementioned limitations. Future investigations of similar nature would tremendously benefit from use of a more well-rounded and balanced data set, given that some classifications in this data set have as few as one or two food items. With a much larger dataset, future experiments could also aim to classify sub-groups, i.e., minor food groups, rather than major.


\newpage
\bibliographystyle{plainnat}
\bibliography{references}


\end{document}
